# -*- coding: utf-8 -*-
"""Untitled0.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1f3VLOofrT01auF8BueH0CXWB69rRsF8X
"""

import tensorflow as tf
from tensorflow.keras import layers, models
import pandas
import numpy as np
from sklearn.model_selection import train_test_split

mapping = {'a': [1,0,0,0,0,0,0,0,0],
        'b': [0,1,0,0,0,0,0,0,0],
        'c': [0,0,1,0,0,0,0,0,0],
        'd': [0,0,0,1,0,0,0,0,0],
        'e': [0,0,0,0,1,0,0,0,0],
        'f': [0,0,0,0,0,1,0,0,0],
        'g': [0,0,0,0,0,0,1,0,0],
        'h': [0,0,0,0,0,0,0,1,0],
        'i': [0,0,0,0,0,0,0,0,1]}

data = pandas.read_csv('./EEdata.csv')

words = data['word']
y = data['word']


x = data.iloc[:,2:]

data.iloc[:,2:]

y = np.concatenate([[mapping[i] for i in y]])

x = x.to_numpy()
x = x[:,:45].reshape(900,5,9)


x = x.transpose(0, 2, 1)
x = np.nan_to_num(x)

encoder = models.Sequential([
    layers.SimpleRNN(64, return_sequences=True, input_shape=(None, 5)),
    layers.SimpleRNN(64)
])
encoder.summary()

decoder = models.Sequential([
    layers.Dense(32, activation='relu', input_shape=(64,)),
    layers.Dense(9, activation='softmax')
])
decoder.summary()


autoencoder = models.Sequential([
    encoder,
    decoder
])
autoencoder.summary()
autoencoder.compile(optimizer='adam', loss='mse', metrics=['accuracy'])

batch_size = 64
epochs = 10000

x_train, x_val, y_train, y_val = train_test_split(x, y, test_size=0.2, random_state=42)
autoencoder.fit(x_train, y_train,
                epochs=epochs,
                batch_size=batch_size,
                shuffle=True,
                validation_data=(x_val, y_val))

autoencoder.evaluate(x_val, y_val)

prediction_array = autoencoder.predict(x[:1])[0]
autoencoder.save('EE_Rnn.keras')

