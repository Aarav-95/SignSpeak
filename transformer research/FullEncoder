# full encoder
# trained  on 800,000 epcohs
# parameters  = 400, 5 heads, 4 layer
# ~0.0016846 training loss and


import numpy as np
import torch
import torch.nn as nn
import pandas
import torch.nn.functional as F
import time

"""Test for 0.2 dropout 5 layers"""

print("gpu usage:", torch.cuda.is_available())
# print(torch.cuda.current_device())
print("gpu name:",torch.cuda.get_device_name(0))
print("memory allocated:",torch.cuda.memory_allocated())
print("memory reserved:", torch.cuda.memory_reserved())

# hyper parameters

vocab_size = 10
n_emb = 5
n_layers = 1
n_heads = 5
n_layers = 5
time_steps = 10
batch_size = 32
epochs = 300000
lr = 1e-4
dropout = 0.2

#encoder layer
data = pandas.read_csv('./Data.csv')
#prepare classes
y = data['word']
stoi  = {s:i for i,s in enumerate(sorted(set(y)))}
encode = lambda s: stoi[s]
y = torch.tensor([encode(s) for i,s in enumerate(y)])
# y = F.one_hot((y.view(y.shape[0], 1)),10) # don't use one hot with cross entropy loss

#prepare data
x = torch.tensor(data.iloc[:,2:time_steps*n_emb + 2].to_numpy())
x = torch.nan_to_num(x).float()
x = x.view(x.shape[0], -1, n_emb)
print(x.dtype, " ", y.dtype)

#shuffle around
indices = np.arange(y.shape[0])
np.random.shuffle(indices)
x = x[indices].cuda()
y = y[indices].cuda()

print(x.shape, " ", y.shape)

class Head(nn.Module):

  def __init__(self):
    super().__init__()
    self.head_size = n_emb // n_heads
    self.keys = nn.Linear(n_emb, self.head_size, bias=False)
    self.queries = nn.Linear(n_emb, self.head_size, bias=False)
    self.values = nn.Linear(n_emb, self.head_size, bias=False)
    self.dropout = nn.Dropout(dropout)

  def forward(self, x):
    k = self.keys(x)
    q = self.queries(x)
    v = self.values(x)

    weights = (q @ torch.transpose(k, 1,2))*(self.head_size)**-0.5
    weights = F.softmax(weights, dim=1)
    weights = self.dropout(weights)
    logits = weights @ v
    # print("logits shape", logits.shape)
    return logits

class MultiHeadAttention(nn.Module):

  def __init__(self):
    super().__init__()
    self.heads = nn.ModuleList([Head().cuda() for i in range(n_heads)])
    self.layer = nn.Sequential(
        nn.Linear(n_emb, n_emb),
        nn.Dropout(dropout)
    )

  def forward(self, x):
    multi_head_attention = [head(x) for head in self.heads]
    # print("multi_head_attention shape  ", multi_head_attention)
    logits = torch.cat(multi_head_attention, -1)
    logits = self.layer(logits)
    return logits

class FeedForwardNetwork(nn.Module):

  def __init__(self):
    super().__init__()
    self.feed_forward = nn.Sequential(
        nn.Linear(n_emb, n_emb*4),
        nn.ReLU(),
        nn.Linear(n_emb*4, n_emb),
        nn.Dropout(dropout)
    )

  def forward(self, x):
    logits = self.feed_forward(x)
    return logits

class Block(nn.Module):

  def __init__(self):
    super().__init__()
    self.heads = MultiHeadAttention()
    self.feed_forward = FeedForwardNetwork()
    self.layer1_norm = nn.LayerNorm(n_emb)
    self.layer2_norm = nn.LayerNorm(n_emb)

  def forward(self, x):
    x = x + self.heads(self.layer1_norm(x))
    logits = x + self.feed_forward(self.layer2_norm(x))

    return logits

class Encoder(nn.Module):

  def __init__(self):
    super().__init__()
    self.embedding_table = nn.Embedding(vocab_size, n_emb)
    self.pos_embedding_table = nn.Embedding(time_steps, n_emb)
    self.blocks = nn.Sequential(*[Block().cuda()for i in range(n_layers)])
    self.final_layer_norm = nn.LayerNorm(n_emb)
    self.linear_output = nn.Linear(n_emb, vocab_size)

  def forward(self, x_input, y_targets):

    pos = self.pos_embedding_table(torch.arange(time_steps).to('cuda'))
    x = x_input + pos
    x = self.blocks(x)
    x = self.final_layer_norm(x)
    x = self.linear_output(x)
    logits = x[:,-1,:].view(x.shape[0],1,vocab_size)
    B,T,C = logits.shape
    logits = logits.view(B*T, C)
    y_targets = y_targets.view(B*T).long()

    loss = F.cross_entropy(logits, y_targets)
    return logits, loss

test_x = x[0].view(1,10,5).cuda()
test_y = y[0].view(1,1).int().to('cuda')
print(test_y.is_cuda)

model = Encoder()
model = model.to('cuda')
parameters = model.parameters()
s = sum([p.nelement() for p in parameters])
# for i in model.parameters():
#   print(i.is_cuda)

logits,loss = model(test_x.cuda(), test_y.cuda())
print(logits, " ", loss)

n = int(0.9*x.shape[0])
Xtr, Ytr = x[:n], y[:n]
Xval, Yval = x[n:], y[n:]

def get_batches(split):
  x_values, y_values = {
      'train': [Xtr, Ytr],
      'test': [Xval, Yval]
  }[split]
  idx = torch.randint(0, x_values.shape[0], (batch_size,))
  return x_values[idx], y_values[idx]

@torch.no_grad()
def get_val_loss(x_val, y_val):
  model.eval()
  logits, val_loss = model(x_val, y_val)
  model.train()
  return val_loss.item()

optim = torch.optim.AdamW(model.parameters(), lr)
start_time = time.time()
for _ in range(epochs):

  x_epoch, y_epoch = get_batches('train')
  logits, loss = model(x_epoch, y_epoch)
  optim.zero_grad(set_to_none=True)
  loss.backward()
  optim.step()
  if(_ % 1000 == 0):
    end_time = time.time()
    delta_time = end_time - start_time
    val_loss = get_val_loss(Xval, Yval)
    print("epoch", _, "   Training Loss", loss.item(), "     Val loss", val_loss, "     Time", delta_time)

    start_time = time.time()

print(loss.item())

# testing

x_test, y_test = get_batches('test')
logits, loss = model(x_test, y_test)
print(loss.item())
